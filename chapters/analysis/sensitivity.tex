\section{Expected Sensitivity to Appearance}

\label{subsec:fitter}
\subsection{Fitting Code}
\improvement{Maybe all of this oscfit stuff should just be moved to just before the systematics section.}

Checks in this analysis are first performed using solely simulation files.
In order to understand the expected sensitivity of this analysis, \improvement{should i even talk about oscfit itself? it seems a bit awkward} {a fitting package previously used to fit the $\nu_\mu$ disappearance} \findref{msu and desy disappearance}.
The code, known as \textbf{OscFit}, works in multiple stages. \needfig{Flowchart of oscfit fitting. at least broadly}
After separating the simulation into separate channels consisting of $\nu_e^{CC}$, $\nu_\mu^{CC}$, $\nu_\tau^{CC}$, $\nu^{NC}$, $\mu_{atm}$, and accidental triggers, the analytic systematics are applied.
These systematics solely rely on information about the particle interaction in order to calculate correction factors to the event weights and are not sensitive to the order of application.
The oscillation calculations are performed at this stage and are based on the Prob3++ code \findref{prob3++} to calclulate the full three-flavor unitary oscillations including matter effects within the Earth.

When including the neutral current interactions from $\nu_\tau$ events in the signal definition, the neutral current events are reweighted for oscillations at this stage.
The OscFit code assumes the neutral current interaction rate is unaffected by oscillations and the $\nu_\tau^{NC}$ events are not directly included in favor of the significantly higher simulation statistics from the other sets.
Because no charged leptons are produced in the neutral current interactions, no differences in event topology are expected based on flavor of neutrino interaction.
For the purposes of this analysis, the neutral current interactions from $\nu_e$ and $\nu_\mu$ events are instead used to model the effect of the $\nu_\tau^{NC}$ events.
The Prob3++ code calculates oscillation probabilities for these events given the expected contribution to the neutral current event rate from $\nu_\tau^{NC}$ events.

\begin{equation}
	R_{\nu_\tau^{NC}} = R_{\nu_e^{NC}} P_{\nu_e\rightarrow\nu_\tau}\left(\theta_{23},\Delta m^2_{3i}\right) +  R_{\nu_\mu^{NC}} P_{\nu_\mu\rightarrow\nu_\tau}\left(\theta_{23},\Delta m^2_{3i}\right)
\end{equation}

The modification to the total neutral current rate given the $\nu_\tau$ normalization, $N_{\nu_\tau}$, is then given by

\label{eqn:nutau_nc}
\begin{equation}
	 R\prime_{\nu^{NC}} = R_{\nu^{NC}} +  R_{\nu_\tau^{NC}} \left( N_{\nu_\tau} - 1\right) 
\end{equation}

The modified weights are then used to histogram the simulated event samplesinto one histogram per simulation channel.
After histogramming, the detector systematics are applied to the each of the binned templates bin-by-bin using hyperplanes calculated for each bin as described in \ref{subsubsec:hyperplanes}.
The hyperplanes themselves are created using the same process, but are created only once using the baseline systematics values and oscillation parameters taken from \findref{nufit 2.2}.
More in-depth tests have shown little change when accounting for changes in the hyperplane coefficients as a function of different oscillation parameters.

Once all systematics have been applied, the normalization terms representing the overall scale factors for the neutrino rate, $N_\nu$, the muon rate, $N_\mu$, and the accidental rate, $N_{noise}$, are mutliplied to the respective histograms.
The final histograms are summed together to form the final simulation expectation to be compared to the data using the $\chi^2_{FS}$ described in \ref{eqn:chi2_final}.

The value of the $\chi^2_{FS}$ is minimized as a function of the various systematics using the iMinuit2 \findref{iminuit} package.
The minimization continues until the requested tolerance, $10^{-16}$, is reached by the minimizer, after which the best fit histogram and systematics values are returned to the user.

\subsection{Sensitivity of the Analysis}
To evaluate the expected sensitivity of this analysis, the OscFit code is used to find the best-fit value of the $\chi^2_{FS}$.
Two methods are used to evaluate both the average expected sensitivity and range of variation of the sensitivity due to both the data and simulation statistics.

The first method, known as the \textbf{Asimov} expectation \findref{asimov expectation?}, begins by creating the expected histogram using baseline values of the systematics and oscillations.
The produced histogram, representing an exact PDF of the expected events, is then used as an estimate of the data.
The $\chi^2_{FS}$ is then minimized while the value of $N_{\nu_{\tau}}$ is fixed at regularly spaced points in the interval [0,2] in order to produce a contour.
A final minimization is performed allowing the minimizer to identify the global best-fit value of $N_{\nu_{\tau}}$.

The final expected sensitivity in the Asimov approach is given by calculating the difference between the values of the $\chi^2_{FS}$ at each point and the global best fit.

\label{eqn:delta_chi2}
\begin{equation}
	\Delta \chi^2 \left(N_{\nu_{\tau}}\right) = \chi^2_{FS}\left(N_{\nu_{\tau}}\right)  - \chi^2_{FS}\left(N^{Global}_{\nu_{\tau}}\right)
\end{equation}

The value of $\Delta \chi^2_{FS}$ as a function of $N_{\nu_{\tau}}$ is shown in \needfig{asimov sensitivity}.
These values may be converted into expected significance levels using Wilk's theorem \findref{wilks theorem} assuming one degree of freedom.

The second method, producing what is known as a \textbf{Brazilian flag} plot due to the color scheme, provides an estimate of the expected range of sensitivities in this analysis.
The production of a Brazilian flag begins with the production of a pseudo-data histogram from the Asimov histogram.
Because the simulation sets used here have significant uncertainties due to limited simulation statistics, the first step is to vary the event rate in each bin within the statistical uncertainties of the Monte Carlo.
To do so, histograms of the uncertainty in each bin are produced using the baseline systematics values

\begin{equation}
	\sigma^{MC}_{ijk} = \sqrt{\sum_m^{Evts} w^2_{ijkm}}
\end{equation}

This uncertainty is assumed to be approximately Gaussian.
The event rate in each bin of each simulation template is then varied using a Gaussian distribution using the expected rate as the mean and $\sigma^{MC}$ as the uncertainty.
The new templates are them summed together and each bin is fluctuated around the new expectation assuming Poisson statistics, creating a representation of one possible realization of the data in the analysis.
The OscFit minimization then proceeds as described in the Asimov case using each of 500 realizations of pseudo-data, with the calculation of the $\Delta \chi^2$ as described in \ref{eqn:delta_chi2}.
The Brazilian flag shows the 1$\sigma$ and 2$\sigma$ range of $\Delta \chi^2$ values around the median at each value of $N_{\nu_{\tau}}$.
This provides a graphical representation, shown in \needfig{brazilian flag}, of the expected range of variation of the sensitivity given solely statistical uncertainties.

\label{subsec:systematics_impact}
\subsection{Impact of Systematics}
There are various ways to measure the impact of the included systematics in this analysis.
Described here are methods to evaluate, in order of increasing importance, the total systematics impact, the impact of each systematic individually, the correlation between systematics, and the effect of non-baseline values.
Each of these test different aspects of the sensitivity and all are included for completeness.

\subsubsection{Total Systematics Impact}
The total impact of the systematics on the sensitivity may be measured by comparing the total Asimov sensitivity to an Asimov sensitivity calculated using no systematics.
This is shown in \needfig{comparison of stat-only fit to full systematics fit}.
It is clear from the comparison that the analysis is very sensitive to the included systematics set.

\subsubsection{N+1 Tests: Sensitivity of Analysis to Systematic}
A different test is also possible: Instead of calculating likelihoods with no systematics included, a single systematic may be used at a time.
This test, called an N+1 test for the addition of one systematic at a time, yields useful information on a sample's sensitivity to single systematics.
A small change in sensitivity between the no-systematics case above and an N+1 Asimov sensitivity may have two possible explanations.
The first, that the current analysis is unaffected by changes in 


\subsubsection{N-1 Tests: Redundancy Between Systematics}

\subsubsection{"Hidden Potential" Tests: Non-Baseline Values}


\label{subsec:sensitivity_vs_time}
\subsection{Expected Sensitivity over Time}



\label{subsec:wilks}
\subsection{Feldman-Cousins vs WIlk's Theorem}