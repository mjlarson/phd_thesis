\section{Expected Sensitivity to Appearance}

\label{subsec:fitter}
\subsection{Fitting Code}
\improvement{Maybe all of this oscfit stuff should just be moved to just before the systematics section.}

Checks in this analysis are first performed using solely simulation files.
In order to understand the expected sensitivity of this analysis, \improvement{should i even talk about oscfit itself? it seems a bit awkward} {a fitting package previously used to fit the $\nu_\mu$ disappearance} \findref{msu and desy disappearance}.
The code, known as \textbf{OscFit}, works in multiple stages. \needfig{Flowchart of oscfit fitting. at least broadly}
After separating the simulation into separate channels consisting of $\nu_e^{CC}$, $\nu_\mu^{CC}$, $\nu_\tau^{CC}$, $\nu^{NC}$, $\mu_{atm}$, and accidental triggers, the analytic systematics are applied.
These systematics solely rely on information about the particle interaction in order to calculate correction factors to the event weights and are not sensitive to the order of application.
The oscillation calculations are performed at this stage and are based on the Prob3++ code \findref{prob3++} to calclulate the full three-flavor unitary oscillations including matter effects within the Earth.

When including the neutral current interactions from $\nu_\tau$ events in the signal definition, the neutral current events are reweighted for oscillations at this stage.
The OscFit code assumes the neutral current interaction rate is unaffected by oscillations and the $\nu_\tau^{NC}$ events are not directly included in favor of the significantly higher simulation statistics from the other sets.
Because no charged leptons are produced in the neutral current interactions, no differences in event topology are expected based on flavor of neutrino interaction.
For the purposes of this analysis, the neutral current interactions from $\nu_e$ and $\nu_\mu$ events are instead used to model the effect of the $\nu_\tau^{NC}$ events.
The Prob3++ code calculates oscillation probabilities for these events given the expected contribution to the neutral current event rate from $\nu_\tau^{NC}$ events.

\begin{equation}
	R_{\nu_\tau^{NC}} = R_{\nu_e^{NC}} P_{\nu_e\rightarrow\nu_\tau}\left(\theta_{23},\Delta m^2_{3i}\right) +  R_{\nu_\mu^{NC}} P_{\nu_\mu\rightarrow\nu_\tau}\left(\theta_{23},\Delta m^2_{3i}\right)
\end{equation}

The modification to the total neutral current rate given the $\nu_\tau$ normalization, $N_{\nu_\tau}$, is then given by

\label{eqn:nutau_nc}
\begin{equation}
	 R\prime_{\nu^{NC}} = R_{\nu^{NC}} +  R_{\nu_\tau^{NC}} \left( N_{\nu_\tau} - 1\right) 
\end{equation}

The modified weights are then used to histogram the simulated event samplesinto one histogram per simulation channel.
After histogramming, the detector systematics are applied to the each of the binned templates bin-by-bin using hyperplanes calculated for each bin as described in \ref{subsubsec:hyperplanes}.
The hyperplanes themselves are created using the same process, but are created only once using the baseline systematics values and oscillation parameters taken from \findref{nufit 2.2}.
More in-depth tests have shown little change when accounting for changes in the hyperplane coefficients as a function of different oscillation parameters.

Once all systematics have been applied, the normalization terms representing the overall scale factors for the neutrino rate, $N_\nu$, the muon rate, $N_\mu$, and the accidental rate, $N_{noise}$, are mutliplied to the respective histograms.
The final histograms are summed together to form the final simulation expectation to be compared to the data using the $\chi^2_{FS}$ described in \ref{eqn:chi2_final}.

The value of the $\chi^2_{FS}$ is minimized as a function of the various systematics using the iMinuit2 \findref{iminuit} package.
The minimization continues until the requested tolerance, $10^{-16}$, is reached by the minimizer, after which the best fit histogram and systematics values are returned to the user.

\subsection{Sensitivity of the Analysis}
To evaluate the expected sensitivity of this analysis, the OscFit code is used to find the best-fit value of the $\chi^2_{FS}$.
Two methods are used to evaluate both the average expected sensitivity and range of variation of the sensitivity due to both the data and simulation statistics.

The first method, known as the \textbf{Asimov} expectation \findref{asimov expectation?}, begins by creating the expected histogram using baseline values of the systematics and oscillations.
The produced histogram, representing an exact PDF of the expected events, is then used as an estimate of the data.
The $\chi^2_{FS}$ is then minimized while the value of $N_{\nu_{\tau}}$ is fixed at regularly spaced points in the interval [0,2] in order to produce a contour.
A final minimization is performed allowing the minimizer to identify the global best-fit value of $N_{\nu_{\tau}}$.

The final expected sensitivity in the Asimov approach is given by calculating the difference between the values of the $\chi^2_{FS}$ at each point and the global best fit.

\label{eqn:delta_chi2}
\begin{equation}
	\Delta \chi^2 \left(N_{\nu_{\tau}}\right) = \chi^2_{FS}\left(N_{\nu_{\tau}}\right)  - \chi^2_{FS}\left(N^{Global}_{\nu_{\tau}}\right)
\end{equation}

The value of $\Delta \chi^2_{FS}$ as a function of $N_{\nu_{\tau}}$ is shown in \needfig{asimov sensitivity}.
These values may be converted into expected significance levels using Wilk's theorem \findref{wilks theorem} assuming one degree of freedom.

The second method, producing what is known as a \textbf{Brazilian flag} plot due to the color scheme, provides an estimate of the expected range of sensitivities in this analysis.
The production of a Brazilian flag begins with the production of a pseudo-data histogram from the Asimov histogram.
Because the simulation sets used here have significant uncertainties due to limited simulation statistics, the first step is to vary the event rate in each bin within the statistical uncertainties of the Monte Carlo.
To do so, histograms of the uncertainty in each bin are produced using the baseline systematics values

\begin{equation}
	\sigma^{MC}_{ijk} = \sqrt{\sum_m^{Evts} w^2_{ijkm}}
\end{equation}

This uncertainty is assumed to be approximately Gaussian.
The event rate in each bin of each simulation template is then varied using a Gaussian distribution using the expected rate as the mean and $\sigma^{MC}$ as the uncertainty.
The new templates are them summed together and each bin is fluctuated around the new expectation assuming Poisson statistics, creating a representation of one possible realization of the data in the analysis.
The OscFit minimization then proceeds as described in the Asimov case using each of 500 realizations of pseudo-data, with the calculation of the $\Delta \chi^2$ as described in \ref{eqn:delta_chi2}.
The Brazilian flag shows the 1$\sigma$ and 2$\sigma$ range of $\Delta \chi^2$ values around the median at each value of $N_{\nu_{\tau}}$.
This provides a graphical representation, shown in \needfig{brazilian flag}, of the expected range of variation of the sensitivity given solely statistical uncertainties.

\label{subsec:systematics_impact}
\subsection{Impact of Systematics}
There are various ways to measure the impact of the included systematics in this analysis.
Described here are methods to evaluate, in order of increasing importance, the total systematics impact, the impact of each systematic individually, the correlation between systematics, and the effect of non-baseline values.
Each of these test different aspects of the sensitivity and all are included for completeness.

\subsubsection{Total Systematics Impact}
The total impact of the systematics on the sensitivity may be measured by comparing the total Asimov sensitivity to an Asimov sensitivity calculated using no systematics.
This is shown in \needfig{comparison of stat-only fit to full systematics fit}.
It is clear from the comparison that the analysis is very sensitive to the included systematics set.

\subsubsection{N+1 Tests: Sensitivity of Analysis to Systematic}
A different test is also possible: Instead of calculating likelihoods with no systematics included, a single systematic may be used at a time.
This test, called an N+1 test for the addition of one systematic at a time, yields useful information on a sample's sensitivity to single systematics.
A small change in sensitivity between the no-systematics case above and an N+1 Asimov sensitivity may have two possible explanations.
The first that the current analysis is unaffected by changes in the systematic, implying that the systematic may be investigated for removal in the analysis.
The second possibllity is that the systematic may interact with other parameters in order to produce an effect.
The second case is more difficult to diagnose, but further tests may be possible.
\needfig{N+1 tests}

\subsubsection{N-1 Tests: Redundancy Between Systematics}
In contrast to the N+1 tests, N-1 tests start with the full suite of systematics included.
One systematic is then fixed to the baseline value and removed before redoing minimization.
The change in the contour as a result of the removal of systematics allows for the investigation of redundancy between systematics.
If, for example, two systematics have similar effects in the final histogram, then the N-1 test will show no change in sensitivity due to the removal.
It is also possible that the analysis is strongly sensitive to the value of the systematic and is unlikely to move from the baseline value.
These tests can be useful in identifying redundant parameters for removal, although with the caveat that combinations of parameters are not tested.
After removal of multiple redundant parameters, the updated Asimov sensitivity should be tested once again to verify that the combination of removed parameters remains irrelevant for the fit

\needfig{N-1 tests}

\subsubsection{"Hidden Potential" Tests: Non-Baseline Values}
Both the N+1 and N-1 test suffer from a particular flaw.
Both fail to test the analysis for exceptionally strong sensitivity to particular systematics.
In order to identify these parameters, the "hidden potential" test has been proposed.
In this test, the Asimov sensitivity of the full analysis containing all proposed systematics is used as a baseline.
Each systematic is then fixed, one at a time, off of the baseline value before rerunning the minimization.
The parameters with priors tend to be fixed to one standard deviation from the prior mean.
The change in the sensitivity gives an indirect measure of the strength of the systematic effect in the analysis.
If no change is observed, the parameter is likely to be redundant and may be investigated for removal from the analysis.

\needfig{hidden potential martin n-1 tests}.
\unsure{should i be including the dropped dis/theta13 systematics here? and maybe deltacp? otherwise this section feels a bit pointless}

\label{subsec:sensitivity_vs_time}
\subsection{Expected Sensitivity over Time}



\label{subsec:wilks}
\subsection{Feldman-Cousins vs WIlk's Theorem}